{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abiralchy0987/movie_recommendation_system/blob/main/Full_recommendation_system.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "C90vVBxWjttz",
        "outputId": "4057e1b5-8014-41b9-86bd-dba1fe9d1d80"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/tmdb_5000_movies.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-bc96b97cf622>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#reading the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmovies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tmdb_5000_movies.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mcredits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/tmdb_5000_credits.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/tmdb_5000_movies.csv'"
          ]
        }
      ],
      "source": [
        "# Content filtering\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib as plt\n",
        "import os\n",
        "\n",
        "#reading the data\n",
        "movies = pd.read_csv('/content/tmdb_5000_movies.csv')\n",
        "credits = pd.read_csv('/content/tmdb_5000_credits.csv')\n",
        "\n",
        "movies.head(2)\n",
        "\n",
        "#head shows only one row of the dataset because of the parameter (1)\n",
        "credits.head(1)\n",
        "\n",
        "movies.shape\n",
        "\n",
        "#shape shows how many rows and columns are available in the dataset\n",
        "credits.shape\n",
        "\n",
        "#integration of datasets movies and credits\n",
        "movies = movies.merge(credits , on = 'title')\n",
        "\n",
        "movies.head(1)\n",
        "\n",
        "#visualising the integrated datasets\n",
        "#previously movies had 20 columns and credits had four columns and since\n",
        "#it was integrated on the column \"title\" which was same on both the datasets\n",
        "#the new number of columns present is 23\n",
        "movies.shape\n",
        "\n",
        "\n",
        "movies.columns\n",
        "\n",
        "#cleaning data\n",
        "movies = movies[['movie_id','title','overview','genres','keywords','cast','crew']]\n",
        "\n",
        "#data preprocessing\n",
        "#checking for missing values\n",
        "movies.isnull().sum()\n",
        "\n",
        "#dropping the rows with missing value because there is very small number of data with missing value\n",
        "movies.dropna(inplace=True)\n",
        "\n",
        "movies.duplicated().sum()\n",
        "\n",
        "movies.iloc[0]['genres']\n",
        "\n",
        "import ast\n",
        "def convert(text):\n",
        "  l=[]\n",
        "  for i in ast.literal_eval(text):\n",
        "    l.append(i['name'])\n",
        "\n",
        "  return l\n",
        "\n",
        "\n",
        "movies['genres']=movies['genres'].apply(convert)\n",
        "\n",
        "movies['keywords']=movies['keywords'].apply(convert)\n",
        "\n",
        "import ast\n",
        "def convert_cast(text):\n",
        "  l=[]\n",
        "  counter=0\n",
        "  for i in ast.literal_eval(text):\n",
        "    if counter<3:\n",
        "     l.append(i['name'])\n",
        "    counter+=1\n",
        "\n",
        "  return l\n",
        "\n",
        "movies['cast']=movies['cast'].apply(convert_cast)\n",
        "\n",
        "import ast\n",
        "def fetch_director(text):\n",
        "  l=[]\n",
        "  for i in ast.literal_eval(text):\n",
        "    if i['job'] == 'Director':\n",
        "     l.append(i['name'])\n",
        "     break\n",
        "\n",
        "  return l\n",
        "\n",
        "movies['crew']=movies['crew'].apply(fetch_director)\n",
        "\n",
        "\n",
        "\n",
        "movies['overview']=movies['overview'].apply(lambda x:x.split())\n",
        "\n",
        "def remove_space(word):\n",
        "  l = []\n",
        "  for i in word:\n",
        "    l.append(i.replace(\" \",\"\"))\n",
        "  return l\n",
        "\n",
        "\n",
        "movies['cast']=movies['cast'].apply(remove_space)\n",
        "\n",
        "movies['crew']=movies['crew'].apply(remove_space)\n",
        "\n",
        "movies['tags']= movies['overview']+movies['keywords']+movies['genres']+movies['cast']+movies['crew']\n",
        "\n",
        "new_df = movies[['movie_id','title','tags']]\n",
        "\n",
        "new_df['tags'] = new_df['tags'].apply(lambda x: \" \".join(x))\n",
        "\n",
        "new_df['tags'] = new_df['tags'].apply(lambda x:x.lower())\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "def stem(text):\n",
        "  l = []\n",
        "  for i in text.split():\n",
        "    l.append(ps.stem(i))\n",
        "\n",
        "  return \" \".join(l)\n",
        "\n",
        "new_df['tags'] = new_df['tags'].apply(stem)\n",
        "\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "words_clean=[]\n",
        "for word in new_df['tags']:\n",
        " stopwords = nltk.corpus.stopwords.words('english')\n",
        "if word not in stopwords:\n",
        "     l = []\n",
        "for i in word.split():\n",
        "     l.append(ps.stem(i))\n",
        "     words_clean.append(\" \".join(l))\n",
        "else:\n",
        "         words_clean.append(word)\n",
        "         print(words_clean)\n",
        "# def stem(text):\n",
        "#   l = []\n",
        "#   for i in text.split():\n",
        "#     l.append(ps.stem(i))\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv = CountVectorizer(max_features=5000,stop_words='english')\n",
        "vector= cv.fit_transform(new_df['tags']).toarray()\n",
        "\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "similarity = cosine_similarity(vector)\n",
        "similarity\n",
        "\n",
        "new_df[new_df['title'] == 'Spider-Man'].index[0]\n",
        "\n",
        "def recommend(movie):\n",
        "  movie_index = new_df[new_df['title'] == movie].index[0]\n",
        "  distances =sorted(list(enumerate(similarity[movie_index])),reverse=True, key = lambda x: x[1])\n",
        "  for i in distances[1:6]:\n",
        "    print(new_df.iloc[i[0]].title)\n",
        "\n",
        "\n",
        "recommend('Spider-Man')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#collaborative filtering\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "movies_df = pd.read_csv('movies.csv')\n",
        "ratings_df = pd.read_csv('ratings.csv')\n",
        "\n",
        "ratings_df.shape\n",
        "\n",
        "movies_df.head(1)\n",
        "\n",
        "ratings_df.head(1)\n",
        "\n",
        "movies = pd.merge(ratings_df, movies_df, on='movieId')\n",
        "\n",
        "\n",
        "# Remove duplicates if any\n",
        "movies = movies.drop_duplicates()\n",
        "\n",
        "\n",
        "# Create the User-Item Rating Matrix (pivot table)\n",
        "user_item_matrix = movies.pivot(index='userId', columns='movieId', values='rating')\n",
        "\n",
        "# Display the shape and first few entries of the matrix\n",
        "print(user_item_matrix.shape)\n",
        "print(user_item_matrix.head())\n",
        "\n",
        "\n",
        "# Calculate the global average rating\n",
        "global_avg_rating = movies['rating'].mean()\n",
        "\n",
        "# Fill NaN values with the global average rating\n",
        "user_item_matrix_filled = user_item_matrix.fillna(global_avg_rating)\n",
        "\n",
        "\n",
        "user_item_matrix_filled.head(1)\n",
        "\n",
        "!pip install scikit-surprise\n",
        "\n",
        "from surprise import SVD, Dataset, Reader\n",
        "from surprise.model_selection import train_test_split\n",
        "from surprise import accuracy\n",
        "\n",
        "# Prepare data for Surprise\n",
        "reader = Reader(rating_scale=(1, 5))\n",
        "data = Dataset.load_from_df(movies[['userId', 'movieId', 'rating']], reader)\n",
        "\n",
        "# Split data into train and test sets\n",
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "\n",
        "# Initialize SVD model\n",
        "svd = SVD()\n",
        "\n",
        "# Train the model\n",
        "svd.fit(trainset)\n",
        "\n",
        "# Predict ratings on the testset\n",
        "predictions = svd.test(testset)\n",
        "\n",
        "# Evaluate the model using RMSE\n",
        "rmse = accuracy.rmse(predictions)\n",
        "print(f'RMSE: {rmse}')\n",
        "\n",
        "\n",
        "# Predict the rating for a specific user-item pair\n",
        "user_id = 1  # Example user\n",
        "movie_id = 50  # Example movie\n",
        "\n",
        "predicted_rating = svd.predict(user_id, movie_id).est\n",
        "print(f\"Predicted rating for User {user_id} on Movie {movie_id}: {predicted_rating}\")\n",
        "\n",
        "\n",
        "import heapq\n",
        "\n",
        "def recommend_movies(user_id, top_n=10):\n",
        "    \"\"\"Recommends top N movies for a specific user.\n",
        "\n",
        "    Args:\n",
        "        user_id (int): The ID of the user.\n",
        "        top_n (int, optional): The number of movies to recommend. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of movie titles.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get all movie IDs and create a dictionary for movieId -> title lookup\n",
        "    movie_id_to_title = dict(zip(movies['movieId'], movies['title']))\n",
        "\n",
        "    # Get all unique movie IDs\n",
        "    all_movie_ids = movies['movieId'].unique()\n",
        "\n",
        "    # Get the movies that the user has already rated\n",
        "    rated_movies = set(movies[movies['userId'] == user_id]['movieId'])\n",
        "\n",
        "    # Get the unrated movies\n",
        "    unrated_movies = [movie_id for movie_id in all_movie_ids if movie_id not in rated_movies]\n",
        "\n",
        "    # Predict ratings for unrated movies\n",
        "    predictions = [svd.predict(user_id, movie_id) for movie_id in unrated_movies]\n",
        "\n",
        "    # Sort movies by predicted rating (highest first)\n",
        "    recommended_movies = heapq.nlargest(top_n, predictions, key=lambda x: x.est)\n",
        "\n",
        "    # Get movie titles from the top N predictions using the dictionary\n",
        "    movie_titles = [movie_id_to_title[rec.iid] for rec in recommended_movies]\n",
        "\n",
        "    return movie_titles\n",
        "\n",
        "# Example usage:\n",
        "user_id = 1  # Example user ID\n",
        "recommended_movies = recommend_movies(user_id, top_n=10)\n",
        "print(f\"Top 10 movie recommendations for User {user_id}: {recommended_movies}\")\n",
        "\n",
        "user_id_input = input(\"Enter your user ID: \")\n",
        "try:\n",
        "    user_id = int(user_id_input)  # Ensure the user ID is an integer\n",
        "    recommended_movies = recommend_movies(user_id, top_n=10)\n",
        "    print(f\"Top 10 movie recommendations for User {user_id}: {recommended_movies}\")\n",
        "except ValueError:\n",
        "    print(\"Invalid input. Please enter a valid integer for user ID.\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3MwMeX6jovdd",
        "outputId": "51836fec-c6fc-4261-ef6f-55569ee91e49"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(610, 9724)\n",
            "movieId  1       2       3       4       5       6       7       8       \\\n",
            "userId                                                                    \n",
            "1           4.0     NaN     4.0     NaN     NaN     4.0     NaN     NaN   \n",
            "2           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "3           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "4           NaN     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "5           4.0     NaN     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "\n",
            "movieId  9       10      ...  193565  193567  193571  193573  193579  193581  \\\n",
            "userId                   ...                                                   \n",
            "1           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "2           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "3           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "4           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "5           NaN     NaN  ...     NaN     NaN     NaN     NaN     NaN     NaN   \n",
            "\n",
            "movieId  193583  193585  193587  193609  \n",
            "userId                                   \n",
            "1           NaN     NaN     NaN     NaN  \n",
            "2           NaN     NaN     NaN     NaN  \n",
            "3           NaN     NaN     NaN     NaN  \n",
            "4           NaN     NaN     NaN     NaN  \n",
            "5           NaN     NaN     NaN     NaN  \n",
            "\n",
            "[5 rows x 9724 columns]\n",
            "Collecting scikit-surprise\n",
            "  Downloading scikit_surprise-1.1.4.tar.gz (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.4/154.4 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.4.2)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-surprise) (1.13.1)\n",
            "Building wheels for collected packages: scikit-surprise\n",
            "  Building wheel for scikit-surprise (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-surprise: filename=scikit_surprise-1.1.4-cp311-cp311-linux_x86_64.whl size=2505172 sha256=d97b9029f265ad02531c7d0268264ecbaf161d53a8a4dd5057c25ba607863907\n",
            "  Stored in directory: /root/.cache/pip/wheels/2a/8f/6e/7e2899163e2d85d8266daab4aa1cdabec7a6c56f83c015b5af\n",
            "Successfully built scikit-surprise\n",
            "Installing collected packages: scikit-surprise\n",
            "Successfully installed scikit-surprise-1.1.4\n",
            "RMSE: 0.8795\n",
            "RMSE: 0.8795477634196139\n",
            "Predicted rating for User 1 on Movie 50: 4.839766605959795\n",
            "Top 10 movie recommendations for User 1: ['Shawshank Redemption, The (1994)', 'Departed, The (2006)', 'Philadelphia Story, The (1940)', 'Amadeus (1984)', 'Bridge on the River Kwai, The (1957)', 'Shrek (2001)', 'Incredibles, The (2004)', 'Grand Day Out with Wallace and Gromit, A (1989)', 'Cool Hand Luke (1967)', \"Amelie (Fabuleux destin d'Amélie Poulain, Le) (2001)\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install thefuzz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zqH3EcT1srnL",
        "outputId": "b1bcb6ab-135f-41ca-9a04-90555d3e19b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting thefuzz\n",
            "  Downloading thefuzz-0.22.1-py3-none-any.whl.metadata (3.9 kB)\n",
            "Collecting rapidfuzz<4.0.0,>=3.0.0 (from thefuzz)\n",
            "  Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Downloading thefuzz-0.22.1-py3-none-any.whl (8.2 kB)\n",
            "Downloading rapidfuzz-3.12.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, thefuzz\n",
            "Successfully installed rapidfuzz-3.12.1 thefuzz-0.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from thefuzz import fuzz, process\n",
        "\n",
        "def find_closest_movie(title_input):\n",
        "    \"\"\"\n",
        "    Finds the closest matching movie title in new_df using fuzzy matching.\n",
        "    Both the user input and the titles are compared in lowercase.\n",
        "    \"\"\"\n",
        "    all_titles = new_df['title'].tolist()\n",
        "    # Create a lowercase version for fuzzy matching\n",
        "    all_titles_lower = [t.lower() for t in all_titles]\n",
        "    title_input_lower = title_input.lower().strip()\n",
        "\n",
        "    # Get the best match using fuzzy matching\n",
        "    best_match, score = process.extractOne(title_input_lower, all_titles_lower, scorer=fuzz.partial_ratio)\n",
        "\n",
        "    # Accept the match if the score is above a threshold (e.g., 60)\n",
        "    if score > 60:\n",
        "        # Return the original title (with proper casing) corresponding to the best match\n",
        "        match_index = all_titles_lower.index(best_match)\n",
        "        return new_df.iloc[match_index]['title']\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def hybrid_recommend(user_id, movie_title, top_n=10):\n",
        "    \"\"\"\n",
        "    Hybrid recommendation that returns half the recommendations from content-based filtering\n",
        "    and half from collaborative filtering.\n",
        "\n",
        "    Args:\n",
        "        user_id (int): The ID of the user (for collaborative filtering).\n",
        "        movie_title (str): The movie title used for content-based filtering (input is fuzzy-matched).\n",
        "        top_n (int): Total number of recommendations to return.\n",
        "\n",
        "    Returns:\n",
        "        list: A list of recommended movie titles.\n",
        "    \"\"\"\n",
        "    # Determine how many movies to take from each recommender\n",
        "    n_content = top_n // 2\n",
        "    n_collab = top_n - n_content  # This ensures if top_n is odd, collaborative gets the extra slot\n",
        "\n",
        "    # ----------- Content-Based Filtering -----------\n",
        "    # Find the closest matching movie title\n",
        "    matched_movie = find_closest_movie(movie_title)\n",
        "    if not matched_movie:\n",
        "        print(f\"Sorry, we couldn't find a close match for '{movie_title}'. Please try another title.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Using content-based match: '{matched_movie}'\")\n",
        "\n",
        "    # Get the index for the matched movie from new_df\n",
        "    try:\n",
        "        movie_index = new_df[new_df['title'] == matched_movie].index[0]\n",
        "    except IndexError:\n",
        "        print(f\"Movie '{matched_movie}' not found in the dataset.\")\n",
        "        return []\n",
        "\n",
        "    # Get similarity scores for all movies relative to the matched movie\n",
        "    content_scores = list(enumerate(similarity[movie_index]))\n",
        "    # Sort scores in descending order (most similar first)\n",
        "    content_scores = sorted(content_scores, key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Build content-based recommendation list (skip the first entry as it is the movie itself)\n",
        "    content_recs = []\n",
        "    for idx, score in content_scores[1:]:\n",
        "        rec_title = new_df.iloc[idx]['title']\n",
        "        if rec_title not in content_recs:\n",
        "            content_recs.append(rec_title)\n",
        "        if len(content_recs) >= n_content:\n",
        "            break\n",
        "\n",
        "    # ----------- Collaborative Filtering -----------\n",
        "    # Get a larger pool from collaborative filtering so we can remove any duplicates later\n",
        "    collab_pool = recommend_movies(user_id, top_n=top_n * 2)\n",
        "    # Remove any movies that already appeared in the content-based recommendations\n",
        "    collab_recs = [m for m in collab_pool if m not in content_recs]\n",
        "    # Select the top n_collab recommendations\n",
        "    collab_recs = collab_recs[:n_collab]\n",
        "\n",
        "    # ----------- Merge Recommendations -----------\n",
        "    final_recs = content_recs + collab_recs\n",
        "\n",
        "    # In case there are not enough unique recommendations, try to fill from the collaborative pool\n",
        "    if len(final_recs) < top_n:\n",
        "        extra_needed = top_n - len(final_recs)\n",
        "        extra_from_collab = [m for m in collab_pool if m not in final_recs]\n",
        "        final_recs += extra_from_collab[:extra_needed]\n",
        "\n",
        "    return final_recs\n",
        "\n",
        "# ---------------------------\n",
        "# Example Usage\n",
        "# ---------------------------\n",
        "\n",
        "# Get user input\n",
        "try:\n",
        "    user_id = int(input(\"Enter your user ID: \"))\n",
        "except ValueError:\n",
        "    print(\"Invalid user ID. Please enter an integer value.\")\n",
        "    user_id = None\n",
        "\n",
        "if user_id is not None:\n",
        "    movie_title = input(\"Enter a movie title you like: \")\n",
        "\n",
        "    recommendations = hybrid_recommend(user_id, movie_title, top_n=10)\n",
        "\n",
        "    if recommendations:\n",
        "        print(\"\\nHybrid Recommendations (50% Content-Based, 50% Collaborative):\")\n",
        "        for i, title in enumerate(recommendations, 1):\n",
        "            print(f\"{i}. {title}\")\n"
      ],
      "metadata": {
        "id": "vxxdG6frtScZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}